## Gradient Descent
- Gradient descent is a method for unconstrained mathematical optimization. It is a __first-order iterative__ algorithm for __minimizing a differentiable multivariate__ function.
- The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent.
- Conversely, __stepping in the direction of the gradient__ will lead to a trajectory that __maximizes that function__; the procedure is then known as __gradient ascent__.